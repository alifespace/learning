{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d8d6c01",
   "metadata": {},
   "source": [
    "# My Cookbook 01\n",
    "\n",
    "1. 产生一个指定大小的文件；\n",
    "2. 产生随机数\n",
    "3. 产生一个指定大小的随机矩阵；\n",
    "4. 产生一个文件的hash值；\n",
    "5. 对一个矩阵产生hash值；\n",
    "6. 转换一个csv到parquet格式；\n",
    "7. 产生伪记录；\n",
    "8. 测试一个复杂对象的占据内存大小；\n",
    "9. 测试 GCS 的上传速度；\n",
    "10. 测试 GCS 的下载速度；"
   ]
  },
  {
   "cell_type": "code",
   "id": "ca7f54f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:14:47.166516Z",
     "start_time": "2025-07-18T17:14:42.633282Z"
    }
   },
   "source": [
    "import hashlib, csv, pyarrow, os, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from pathlib import Path\n",
    "from faker import Faker\n",
    "from tqdm import tqdm\n",
    "from pympler import asizeof\n",
    "from google.cloud import storage\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T16:26:01.632187Z",
     "start_time": "2025-07-18T16:26:01.625327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_testfile(path: str, size_mb: int = 500):\n",
    "    print(f\"Generating {size_mb}MB test file...\")\n",
    "    with open(path, \"wb\") as f:\n",
    "        f.write(os.urandom(size_mb * 1024 * 1024))\n",
    "    print(f\"File created.\")"
   ],
   "id": "11b183b07150f3ae",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "a1f63264",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T16:10:56.640932Z",
     "start_time": "2025-07-18T16:10:56.629882Z"
    }
   },
   "source": [
    "# 产生随机数\n",
    "def generate_random(seed: int = None):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    return rng.random(size=1)\n",
    "\n",
    "print(f\"seed is None: {generate_random()}\")\n",
    "print(f\"Seed is 42: {generate_random(42)}\")\n",
    "print(f\"seed is 43: {generate_random(43)}\")\n",
    "print(f\"Seed is 42: {generate_random(42)}\")\n",
    "print(f\"data type is: {generate_random().dtype}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed is None: [0.12069404]\n",
      "Seed is 42: [0.77395605]\n",
      "seed is 43: [0.65229926]\n",
      "Seed is 42: [0.77395605]\n",
      "data type is: float64\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb17b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 产生一个指定大小的随机矩阵\n",
    "def matrix_generate(m: int, n: int, seed: int = 7, dtype=np.float64) -> np.ndarray:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    return rng.random(size=(m, n), dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69f55d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 产生一个文件的Hash值\n",
    "def get_file_hash(filename: Path, chunk_size: int = 8192) -> str:\n",
    "    \"\"\"产生一个文件的哈希值\n",
    "\n",
    "    Args:\n",
    "        filename (Path): 文件的Path对象\n",
    "        chunk_size (int, optional): 每次读取的字节数. Defaults to 8192.\n",
    "\n",
    "    Returns:\n",
    "        str: 文件的十六进制哈希字符串\n",
    "    \"\"\"\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    with filename.open(\"rb\") as f:\n",
    "        for byte_block in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            sha256_hash.update(byte_block)\n",
    "\n",
    "    return sha256_hash.hexdigest()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd335064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 产生一个矩阵的hash值\n",
    "def get_matrix_hash(matrix: np.ndarray) -> str:\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    sha256_hash.update(matrix.data)\n",
    "    return sha256_hash.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88f314fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_parquet(input_file: str, output_file: str):\n",
    "    ddf = dd.read_csv(input_file, blocksize='32MB')\n",
    "    ddf.to_parquet(output_file, \n",
    "                   engine='pyarrow', \n",
    "                   compression='snappy', \n",
    "                   write_index=False,\n",
    "                   write_metadata_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97deb003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 产生伪记录\n",
    "def fake_records_generate_EXP(num_records: int = 1_000_000):\n",
    "    fake = Faker(\"zh_CN\")\n",
    "    records = []\n",
    "    for _ in tqdm(range(num_records), desc=\"生成记录：\", unit=\"条\"):\n",
    "        record = {\n",
    "            \"id\": fake.uuid4(),\n",
    "            \"name\": fake.name(),\n",
    "            \"address\": fake.address(),\n",
    "            \"email\": fake.email(),\n",
    "            \"phone_number\": fake.phone_number(),\n",
    "            \"dob\": fake.date_of_birth(minimum_age=18, maximum_age=90),\n",
    "            \"job_title\": fake.job(),\n",
    "            \"company\": fake.company(),\n",
    "            \"text_sample\": fake.paragraph(nb_sentences=2)\n",
    "        }\n",
    "        records.append(record)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef249ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 产生伪记录\n",
    "def fake_records_generate(outfile:str, num_records: int = 100_000):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        outfile (str): 产生大量伪记录后的存储文件位置, 存储格式是csv文件\n",
    "        num_records (int, optional): 产生的记录条数. Defaults to 100_000.\n",
    "    \"\"\"\n",
    "    fake = Faker(\"zh_CN\")\n",
    "    \n",
    "    # 定义一个默认的csv头文件\n",
    "    fieldnames = [\n",
    "        \"id\",\n",
    "        \"name\",\n",
    "        \"address\",\n",
    "        \"email\",\n",
    "        \"phone_number\",\n",
    "        \"dob\",\n",
    "        \"job_title\",\n",
    "        \"company\",\n",
    "        \"text_sample\"\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        with open(outfile, 'w', newline='', encoding='utf-8') as f:\n",
    "            # 定义如何对应字典，未来writer将会把字典对应的值写入到文件中\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "\n",
    "            writer.writeheader()\n",
    "\n",
    "            for _ in tqdm(range(num_records), desc='生成并写入csv文件', unit='条'):\n",
    "                record = {\n",
    "                \"id\": fake.uuid4(),\n",
    "                \"name\": fake.name(),\n",
    "                \"address\": fake.address(),\n",
    "                \"email\": fake.email(),\n",
    "                \"phone_number\": fake.phone_number(),\n",
    "                \"dob\": fake.date_of_birth(minimum_age=18, maximum_age=90),\n",
    "                \"job_title\": fake.job(),\n",
    "                \"company\": fake.ompany(),\n",
    "                \"text_sample\": fake.paragraph(nb_sentences=2)\n",
    "                }\n",
    "\n",
    "                writer.writerow(record)\n",
    "            \n",
    "    except IOError as e:\n",
    "        print(f\"写入文件时发生错误：{e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"发生未知错误：{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc272c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试一个复杂对象的占据内存大小\n",
    "def get_obj_memory_usage(obj):\n",
    "    return asizeof.asizeof(obj)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T16:46:42.771034Z",
     "start_time": "2025-07-18T16:46:42.763788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 测试 GCS 的上传速度\n",
    "def uploadtest_to_gcs(bucket_name: str, local_file: str, blob_file:str):\n",
    "    print(\"Uploading to GCS...\")\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_file)\n",
    "\n",
    "    start = time.time()\n",
    "    blob.upload_from_filename(local_file)\n",
    "    end = time.time()\n",
    "\n",
    "    size_mb= os.stat(local_file).st_size / 1024 ** 2\n",
    "    speed = size_mb / (end - start)\n",
    "\n",
    "    print(f\"Uploaded {size_mb:.2f}MB in {end - start: .2f}s -> {speed: .2f}MB/s\")"
   ],
   "id": "a3b9941c6fa2bc0",
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f219f63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbe35bd3d6c5b30c056e08a4c47302708780638a87a6f389adc85557e77cefea\n",
      "e7cdefd648e806a02a0dc7309ce6bdf88c918cf4891d397393a0e75251ff3240\n",
      "e7cdefd648e806a02a0dc7309ce6bdf88c918cf4891d397393a0e75251ff3240\n",
      "b5f7b96891bd1360677496b6ed11a674b12ec5e81ea48d226bb8f0ac63381d4f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成并写入csv文件: 100%|██████████| 500000/500000 [05:54<00:00, 1411.03条/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex Object A size: 0.00 MB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path1 = Path.cwd()\n",
    "print(get_file_hash(path1 / \"mycookbook_01.ipynb\"))\n",
    "A = matrix_generate(13, 24, dtype=np.float32)\n",
    "print(get_matrix_hash(A))\n",
    "A = matrix_generate(13, 24, dtype=np.float32)\n",
    "print(get_matrix_hash(A))\n",
    "A = matrix_generate(13, 24)\n",
    "print(get_matrix_hash(A))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b7954d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成并写入csv文件:   0%|          | 0/500000 [00:00<?, ?条/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成并写入csv文件: 100%|██████████| 500000/500000 [01:53<00:00, 4394.92条/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas 成功加载数据集, 共500000行。\n",
      "                                     id name               address  \\\n",
      "0  c0106607-5b74-458b-a12b-95a2fd9cff81  杨丽丽  黑龙江省郑州县和平梁街z座 230248   \n",
      "1  2a1fa1ec-4572-4f3b-b874-50bac728cd3b   邱芳    北京市莉县高明张路x座 490153   \n",
      "2  27e8c165-23e0-42b8-a761-3cbbc77fd217   张军  吉林省勇市上街六盘水路G座 145991   \n",
      "3  c44809f2-3fe0-43fe-b372-e12090a41a3b  刘冬梅    安徽省畅市清河李街O座 267239   \n",
      "4  83ea41f4-30a7-415d-ae86-c70982bfcca2   刘刚   天津市合山市海陵白路E座 409198   \n",
      "\n",
      "                 email  phone_number         dob      job_title      company  \\\n",
      "0      hma@example.net   13091432729  1957-10-01  咨询热线/呼叫中心服务人员   恒聪百汇科技有限公司   \n",
      "1    wanli@example.com   14763462227  1953-03-22      药品生产/质量管理     群英网络有限公司   \n",
      "2    wwang@example.com   15269476972  1942-09-14        经理助理/秘书    飞利信科技有限公司   \n",
      "3  sunyong@example.net   18999401551  1987-01-26           公关经理   通际名联科技有限公司   \n",
      "4   yijuan@example.org   13456281956  1966-10-02          订单处理员  时空盒数字信息有限公司   \n",
      "\n",
      "                  text_sample  \n",
      "0           程序成为系统市场城市是否不是上海.  \n",
      "1  得到时候因此本站今天信息.一样之后她的设计信息位置.  \n",
      "2             觉得应用只是合作会员精华大家.  \n",
      "3        觉得其中实现不能.全部政府她的日期学校.  \n",
      "4  免费积分得到这个.那么然后很多为了注册社会国内类别.  \n"
     ]
    }
   ],
   "source": [
    "A = fake_records_generate('fake_test.csv', 500_000)\n",
    "# print(f\"Complex Object A size: {get_obj_memory_usage(A)/1024**2:.2f} MB.\")\n",
    "\n",
    "csv_to_parquet('fake_test.csv', 'fake_test.parquet')\n",
    "\n",
    "df = pd.read_parquet('fake_test.parquet', engine='pyarrow')\n",
    "print(f\"Pandas 成功加载数据集, 共{len(df)}行。\")\n",
    "print(df.head())"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T16:48:04.850790Z",
     "start_time": "2025-07-18T16:47:15.932668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# generate_testfile(\"test\", 200)\n",
    "\n",
    "buck_name = \"celltrix-bucket-01\"\n",
    "blob_name = \"datasets/test_200MB.dat\"\n",
    "local_name= \"test_200MB.dat\"\n",
    "\n",
    "uploadtest_to_gcs(buck_name, local_name, blob_name)"
   ],
   "id": "31c18273cb71d8b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading to GCS...\n",
      "Uploaded 200.00MB in  45.29s ->  4.42MB/s\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
